{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm.models import resnet18, resnet101c, efficientnetv2_s, efficientnetv2_m, vit_tiny_r_s16_p8_384, swinv2_cr_small_384, swinv2_cr_tiny_384\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     StdConv2dSame-1         [-1, 64, 192, 192]           9,408\n",
      "          Identity-2         [-1, 64, 192, 192]               0\n",
      "              ReLU-3         [-1, 64, 192, 192]               0\n",
      "      GroupNormAct-4         [-1, 64, 192, 192]             128\n",
      "     MaxPool2dSame-5           [-1, 64, 96, 96]               0\n",
      "            Conv2d-6          [-1, 192, 12, 12]         786,624\n",
      "       HybridEmbed-7             [-1, 144, 192]               0\n",
      "           Dropout-8             [-1, 145, 192]               0\n",
      "          Identity-9             [-1, 145, 192]               0\n",
      "         Identity-10             [-1, 145, 192]               0\n",
      "        LayerNorm-11             [-1, 145, 192]             384\n",
      "           Linear-12             [-1, 145, 576]         111,168\n",
      "         Identity-13           [-1, 3, 145, 64]               0\n",
      "         Identity-14           [-1, 3, 145, 64]               0\n",
      "           Linear-15             [-1, 145, 192]          37,056\n",
      "          Dropout-16             [-1, 145, 192]               0\n",
      "        Attention-17             [-1, 145, 192]               0\n",
      "         Identity-18             [-1, 145, 192]               0\n",
      "         Identity-19             [-1, 145, 192]               0\n",
      "        LayerNorm-20             [-1, 145, 192]             384\n",
      "           Linear-21             [-1, 145, 768]         148,224\n",
      "             GELU-22             [-1, 145, 768]               0\n",
      "          Dropout-23             [-1, 145, 768]               0\n",
      "           Linear-24             [-1, 145, 192]         147,648\n",
      "          Dropout-25             [-1, 145, 192]               0\n",
      "              Mlp-26             [-1, 145, 192]               0\n",
      "         Identity-27             [-1, 145, 192]               0\n",
      "         Identity-28             [-1, 145, 192]               0\n",
      "            Block-29             [-1, 145, 192]               0\n",
      "        LayerNorm-30             [-1, 145, 192]             384\n",
      "           Linear-31             [-1, 145, 576]         111,168\n",
      "         Identity-32           [-1, 3, 145, 64]               0\n",
      "         Identity-33           [-1, 3, 145, 64]               0\n",
      "           Linear-34             [-1, 145, 192]          37,056\n",
      "          Dropout-35             [-1, 145, 192]               0\n",
      "        Attention-36             [-1, 145, 192]               0\n",
      "         Identity-37             [-1, 145, 192]               0\n",
      "         Identity-38             [-1, 145, 192]               0\n",
      "        LayerNorm-39             [-1, 145, 192]             384\n",
      "           Linear-40             [-1, 145, 768]         148,224\n",
      "             GELU-41             [-1, 145, 768]               0\n",
      "          Dropout-42             [-1, 145, 768]               0\n",
      "           Linear-43             [-1, 145, 192]         147,648\n",
      "          Dropout-44             [-1, 145, 192]               0\n",
      "              Mlp-45             [-1, 145, 192]               0\n",
      "         Identity-46             [-1, 145, 192]               0\n",
      "         Identity-47             [-1, 145, 192]               0\n",
      "            Block-48             [-1, 145, 192]               0\n",
      "        LayerNorm-49             [-1, 145, 192]             384\n",
      "           Linear-50             [-1, 145, 576]         111,168\n",
      "         Identity-51           [-1, 3, 145, 64]               0\n",
      "         Identity-52           [-1, 3, 145, 64]               0\n",
      "           Linear-53             [-1, 145, 192]          37,056\n",
      "          Dropout-54             [-1, 145, 192]               0\n",
      "        Attention-55             [-1, 145, 192]               0\n",
      "         Identity-56             [-1, 145, 192]               0\n",
      "         Identity-57             [-1, 145, 192]               0\n",
      "        LayerNorm-58             [-1, 145, 192]             384\n",
      "           Linear-59             [-1, 145, 768]         148,224\n",
      "             GELU-60             [-1, 145, 768]               0\n",
      "          Dropout-61             [-1, 145, 768]               0\n",
      "           Linear-62             [-1, 145, 192]         147,648\n",
      "          Dropout-63             [-1, 145, 192]               0\n",
      "              Mlp-64             [-1, 145, 192]               0\n",
      "         Identity-65             [-1, 145, 192]               0\n",
      "         Identity-66             [-1, 145, 192]               0\n",
      "            Block-67             [-1, 145, 192]               0\n",
      "        LayerNorm-68             [-1, 145, 192]             384\n",
      "           Linear-69             [-1, 145, 576]         111,168\n",
      "         Identity-70           [-1, 3, 145, 64]               0\n",
      "         Identity-71           [-1, 3, 145, 64]               0\n",
      "           Linear-72             [-1, 145, 192]          37,056\n",
      "          Dropout-73             [-1, 145, 192]               0\n",
      "        Attention-74             [-1, 145, 192]               0\n",
      "         Identity-75             [-1, 145, 192]               0\n",
      "         Identity-76             [-1, 145, 192]               0\n",
      "        LayerNorm-77             [-1, 145, 192]             384\n",
      "           Linear-78             [-1, 145, 768]         148,224\n",
      "             GELU-79             [-1, 145, 768]               0\n",
      "          Dropout-80             [-1, 145, 768]               0\n",
      "           Linear-81             [-1, 145, 192]         147,648\n",
      "          Dropout-82             [-1, 145, 192]               0\n",
      "              Mlp-83             [-1, 145, 192]               0\n",
      "         Identity-84             [-1, 145, 192]               0\n",
      "         Identity-85             [-1, 145, 192]               0\n",
      "            Block-86             [-1, 145, 192]               0\n",
      "        LayerNorm-87             [-1, 145, 192]             384\n",
      "           Linear-88             [-1, 145, 576]         111,168\n",
      "         Identity-89           [-1, 3, 145, 64]               0\n",
      "         Identity-90           [-1, 3, 145, 64]               0\n",
      "           Linear-91             [-1, 145, 192]          37,056\n",
      "          Dropout-92             [-1, 145, 192]               0\n",
      "        Attention-93             [-1, 145, 192]               0\n",
      "         Identity-94             [-1, 145, 192]               0\n",
      "         Identity-95             [-1, 145, 192]               0\n",
      "        LayerNorm-96             [-1, 145, 192]             384\n",
      "           Linear-97             [-1, 145, 768]         148,224\n",
      "             GELU-98             [-1, 145, 768]               0\n",
      "          Dropout-99             [-1, 145, 768]               0\n",
      "          Linear-100             [-1, 145, 192]         147,648\n",
      "         Dropout-101             [-1, 145, 192]               0\n",
      "             Mlp-102             [-1, 145, 192]               0\n",
      "        Identity-103             [-1, 145, 192]               0\n",
      "        Identity-104             [-1, 145, 192]               0\n",
      "           Block-105             [-1, 145, 192]               0\n",
      "       LayerNorm-106             [-1, 145, 192]             384\n",
      "          Linear-107             [-1, 145, 576]         111,168\n",
      "        Identity-108           [-1, 3, 145, 64]               0\n",
      "        Identity-109           [-1, 3, 145, 64]               0\n",
      "          Linear-110             [-1, 145, 192]          37,056\n",
      "         Dropout-111             [-1, 145, 192]               0\n",
      "       Attention-112             [-1, 145, 192]               0\n",
      "        Identity-113             [-1, 145, 192]               0\n",
      "        Identity-114             [-1, 145, 192]               0\n",
      "       LayerNorm-115             [-1, 145, 192]             384\n",
      "          Linear-116             [-1, 145, 768]         148,224\n",
      "            GELU-117             [-1, 145, 768]               0\n",
      "         Dropout-118             [-1, 145, 768]               0\n",
      "          Linear-119             [-1, 145, 192]         147,648\n",
      "         Dropout-120             [-1, 145, 192]               0\n",
      "             Mlp-121             [-1, 145, 192]               0\n",
      "        Identity-122             [-1, 145, 192]               0\n",
      "        Identity-123             [-1, 145, 192]               0\n",
      "           Block-124             [-1, 145, 192]               0\n",
      "       LayerNorm-125             [-1, 145, 192]             384\n",
      "          Linear-126             [-1, 145, 576]         111,168\n",
      "        Identity-127           [-1, 3, 145, 64]               0\n",
      "        Identity-128           [-1, 3, 145, 64]               0\n",
      "          Linear-129             [-1, 145, 192]          37,056\n",
      "         Dropout-130             [-1, 145, 192]               0\n",
      "       Attention-131             [-1, 145, 192]               0\n",
      "        Identity-132             [-1, 145, 192]               0\n",
      "        Identity-133             [-1, 145, 192]               0\n",
      "       LayerNorm-134             [-1, 145, 192]             384\n",
      "          Linear-135             [-1, 145, 768]         148,224\n",
      "            GELU-136             [-1, 145, 768]               0\n",
      "         Dropout-137             [-1, 145, 768]               0\n",
      "          Linear-138             [-1, 145, 192]         147,648\n",
      "         Dropout-139             [-1, 145, 192]               0\n",
      "             Mlp-140             [-1, 145, 192]               0\n",
      "        Identity-141             [-1, 145, 192]               0\n",
      "        Identity-142             [-1, 145, 192]               0\n",
      "           Block-143             [-1, 145, 192]               0\n",
      "       LayerNorm-144             [-1, 145, 192]             384\n",
      "          Linear-145             [-1, 145, 576]         111,168\n",
      "        Identity-146           [-1, 3, 145, 64]               0\n",
      "        Identity-147           [-1, 3, 145, 64]               0\n",
      "          Linear-148             [-1, 145, 192]          37,056\n",
      "         Dropout-149             [-1, 145, 192]               0\n",
      "       Attention-150             [-1, 145, 192]               0\n",
      "        Identity-151             [-1, 145, 192]               0\n",
      "        Identity-152             [-1, 145, 192]               0\n",
      "       LayerNorm-153             [-1, 145, 192]             384\n",
      "          Linear-154             [-1, 145, 768]         148,224\n",
      "            GELU-155             [-1, 145, 768]               0\n",
      "         Dropout-156             [-1, 145, 768]               0\n",
      "          Linear-157             [-1, 145, 192]         147,648\n",
      "         Dropout-158             [-1, 145, 192]               0\n",
      "             Mlp-159             [-1, 145, 192]               0\n",
      "        Identity-160             [-1, 145, 192]               0\n",
      "        Identity-161             [-1, 145, 192]               0\n",
      "           Block-162             [-1, 145, 192]               0\n",
      "       LayerNorm-163             [-1, 145, 192]             384\n",
      "          Linear-164             [-1, 145, 576]         111,168\n",
      "        Identity-165           [-1, 3, 145, 64]               0\n",
      "        Identity-166           [-1, 3, 145, 64]               0\n",
      "          Linear-167             [-1, 145, 192]          37,056\n",
      "         Dropout-168             [-1, 145, 192]               0\n",
      "       Attention-169             [-1, 145, 192]               0\n",
      "        Identity-170             [-1, 145, 192]               0\n",
      "        Identity-171             [-1, 145, 192]               0\n",
      "       LayerNorm-172             [-1, 145, 192]             384\n",
      "          Linear-173             [-1, 145, 768]         148,224\n",
      "            GELU-174             [-1, 145, 768]               0\n",
      "         Dropout-175             [-1, 145, 768]               0\n",
      "          Linear-176             [-1, 145, 192]         147,648\n",
      "         Dropout-177             [-1, 145, 192]               0\n",
      "             Mlp-178             [-1, 145, 192]               0\n",
      "        Identity-179             [-1, 145, 192]               0\n",
      "        Identity-180             [-1, 145, 192]               0\n",
      "           Block-181             [-1, 145, 192]               0\n",
      "       LayerNorm-182             [-1, 145, 192]             384\n",
      "          Linear-183             [-1, 145, 576]         111,168\n",
      "        Identity-184           [-1, 3, 145, 64]               0\n",
      "        Identity-185           [-1, 3, 145, 64]               0\n",
      "          Linear-186             [-1, 145, 192]          37,056\n",
      "         Dropout-187             [-1, 145, 192]               0\n",
      "       Attention-188             [-1, 145, 192]               0\n",
      "        Identity-189             [-1, 145, 192]               0\n",
      "        Identity-190             [-1, 145, 192]               0\n",
      "       LayerNorm-191             [-1, 145, 192]             384\n",
      "          Linear-192             [-1, 145, 768]         148,224\n",
      "            GELU-193             [-1, 145, 768]               0\n",
      "         Dropout-194             [-1, 145, 768]               0\n",
      "          Linear-195             [-1, 145, 192]         147,648\n",
      "         Dropout-196             [-1, 145, 192]               0\n",
      "             Mlp-197             [-1, 145, 192]               0\n",
      "        Identity-198             [-1, 145, 192]               0\n",
      "        Identity-199             [-1, 145, 192]               0\n",
      "           Block-200             [-1, 145, 192]               0\n",
      "       LayerNorm-201             [-1, 145, 192]             384\n",
      "          Linear-202             [-1, 145, 576]         111,168\n",
      "        Identity-203           [-1, 3, 145, 64]               0\n",
      "        Identity-204           [-1, 3, 145, 64]               0\n",
      "          Linear-205             [-1, 145, 192]          37,056\n",
      "         Dropout-206             [-1, 145, 192]               0\n",
      "       Attention-207             [-1, 145, 192]               0\n",
      "        Identity-208             [-1, 145, 192]               0\n",
      "        Identity-209             [-1, 145, 192]               0\n",
      "       LayerNorm-210             [-1, 145, 192]             384\n",
      "          Linear-211             [-1, 145, 768]         148,224\n",
      "            GELU-212             [-1, 145, 768]               0\n",
      "         Dropout-213             [-1, 145, 768]               0\n",
      "          Linear-214             [-1, 145, 192]         147,648\n",
      "         Dropout-215             [-1, 145, 192]               0\n",
      "             Mlp-216             [-1, 145, 192]               0\n",
      "        Identity-217             [-1, 145, 192]               0\n",
      "        Identity-218             [-1, 145, 192]               0\n",
      "           Block-219             [-1, 145, 192]               0\n",
      "       LayerNorm-220             [-1, 145, 192]             384\n",
      "          Linear-221             [-1, 145, 576]         111,168\n",
      "        Identity-222           [-1, 3, 145, 64]               0\n",
      "        Identity-223           [-1, 3, 145, 64]               0\n",
      "          Linear-224             [-1, 145, 192]          37,056\n",
      "         Dropout-225             [-1, 145, 192]               0\n",
      "       Attention-226             [-1, 145, 192]               0\n",
      "        Identity-227             [-1, 145, 192]               0\n",
      "        Identity-228             [-1, 145, 192]               0\n",
      "       LayerNorm-229             [-1, 145, 192]             384\n",
      "          Linear-230             [-1, 145, 768]         148,224\n",
      "            GELU-231             [-1, 145, 768]               0\n",
      "         Dropout-232             [-1, 145, 768]               0\n",
      "          Linear-233             [-1, 145, 192]         147,648\n",
      "         Dropout-234             [-1, 145, 192]               0\n",
      "             Mlp-235             [-1, 145, 192]               0\n",
      "        Identity-236             [-1, 145, 192]               0\n",
      "        Identity-237             [-1, 145, 192]               0\n",
      "           Block-238             [-1, 145, 192]               0\n",
      "       LayerNorm-239             [-1, 145, 192]             384\n",
      "        Identity-240                  [-1, 192]               0\n",
      "         Dropout-241                  [-1, 192]               0\n",
      "          Linear-242                 [-1, 1000]         193,000\n",
      "================================================================\n",
      "Total params: 6,327,912\n",
      "Trainable params: 6,327,912\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.69\n",
      "Forward/backward pass size (MB): 154.25\n",
      "Params size (MB): 24.14\n",
      "Estimated Total Size (MB): 180.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = vit_tiny_r_s16_p8_384(pretrained=False).cuda()\n",
    "summary(model, (3, 384, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet101c().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnetv2_s().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnetv2_m().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.Normalize(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "            \"/opt/ml/level3_cv_finalproject-cv-01/model\", is_train=True, tf=tf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "rcp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, ingredients in train_loader:\n",
    "        images, labels, ingredients = (\n",
    "                images.clone().detach().cuda(),\n",
    "                labels.clone().detach().cuda(),\n",
    "                ingredients.clone().detach().cuda(),\n",
    "            )\n",
    "        print(f\"image : {images.shape}\")\n",
    "        print(f\"label : {labels.shape}\")\n",
    "        print(f\"ingredient : {ingredients.shape}\")\n",
    "        print(f\"max : {torch.max(images)}, min : {torch.min(images)}\")\n",
    "\n",
    "        cls_output, rcp_output = model(images)\n",
    "        print(f\"cls_output : {cls_output.shape}\")\n",
    "        print(f\"rcp_output : {rcp_output.shape}\")\n",
    "        print(f\"max : {torch.max(cls_output)}, min : {torch.min(cls_output)}\")\n",
    "        print(f\"max : {torch.max(rcp_output)}, min : {torch.min(rcp_output)}\")\n",
    "\n",
    "        cls_output = F.sigmoid(cls_output)\n",
    "        rcp_output = F.sigmoid(rcp_output)\n",
    "        print(f\"max : {torch.max(cls_output)}, min : {torch.min(cls_output)}\")\n",
    "        print(f\"max : {torch.max(rcp_output)}, min : {torch.min(rcp_output)}\")\n",
    "        print(f\"max : {torch.max(labels)}, min : {torch.min(labels)}\")\n",
    "        print(f\"max : {torch.max(ingredients)}, min : {torch.min(ingredients)}\")\n",
    "\n",
    "        cls_loss = cls_criterion(cls_output, labels)\n",
    "        rcp_loss = rcp_criterion(rcp_output, ingredients)\n",
    "        print(f\"Loss : {cls_loss}, {rcp_loss}\")\n",
    "\n",
    "        cls_thr = 0.5\n",
    "        rcp_thr = 0.5\n",
    "        cls_output = (cls_output >= cls_thr).float()\n",
    "        rcp_output = (rcp_output >= cls_thr).float()\n",
    "        cls_loss = cls_criterion(cls_output, labels)\n",
    "        rcp_loss = rcp_criterion(rcp_output, ingredients)\n",
    "        print(f\"Loss : {cls_loss}, {rcp_loss}\")\n",
    "\n",
    "        cls_f1 = f1_score(y_pred=cls_output.cpu(), y_true=labels.cpu(), average=\"macro\", zero_division=0)\n",
    "        rcp_f1 = f1_score(y_pred=rcp_output.cpu(), y_true=ingredients.cpu(), average=\"macro\", zero_division=0)\n",
    "        cls_precision = precision_score(y_pred=cls_output.cpu(), y_true=labels.cpu(), average=\"macro\", zero_division=0)\n",
    "        rcp_precision = precision_score(y_pred=rcp_output.cpu(), y_true=ingredients.cpu(), average=\"macro\", zero_division=0)\n",
    "        print(f\"F1-Score : {cls_f1}, {rcp_f1}\")\n",
    "        print(f\"Precision Score : {cls_precision}, {rcp_precision}\")\n",
    "        raise\n",
    "        for i in range(images.shape[0]):\n",
    "            image = images[i].cpu()\n",
    "            image = image.permute(1, 2, 0)\n",
    "            image_np = image.numpy()\n",
    "            print(train_dataset.classes[labels[i].item()])\n",
    "            plt.imshow(image_np)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import cuda\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "max_epoch = 50\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "for epoch in range(max_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    lrs.append(current_lr)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lrs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "path = \"/opt/ml/level3_cv_finalproject-cv-01/model/data/json/milfeuille\"\n",
    "file_names = os.listdir(path)\n",
    "for file_name in file_names:\n",
    "    # new_name = file_name.replace(\"onirigi\", \"onigiri\")\n",
    "    with open(os.path.join(path, file_name)) as file:\n",
    "        data = json.load(file)\n",
    "    data['food_class'] = \"milfeuille\"\n",
    "    with open(os.path.join(\"/opt/ml/level3_cv_finalproject-cv-01/model/milfeuille\", file_name), \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
