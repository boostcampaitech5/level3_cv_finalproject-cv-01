{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import wandb\n",
    "import datetime\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from timm.models import resnet18, resnet101c, efficientnetv2_s, efficientnetv2_m, vit_tiny_r_s16_p8_384, vit_small_r26_s32_384\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "     StdConv2dSame-1         [-1, 64, 192, 192]           9,408\n",
      "          Identity-2         [-1, 64, 192, 192]               0\n",
      "              ReLU-3         [-1, 64, 192, 192]               0\n",
      "      GroupNormAct-4         [-1, 64, 192, 192]             128\n",
      "     MaxPool2dSame-5           [-1, 64, 96, 96]               0\n",
      "     StdConv2dSame-6          [-1, 256, 96, 96]          16,384\n",
      "          Identity-7          [-1, 256, 96, 96]               0\n",
      "          Identity-8          [-1, 256, 96, 96]               0\n",
      "      GroupNormAct-9          [-1, 256, 96, 96]             512\n",
      "   DownsampleConv-10          [-1, 256, 96, 96]               0\n",
      "    StdConv2dSame-11           [-1, 64, 96, 96]           4,096\n",
      "         Identity-12           [-1, 64, 96, 96]               0\n",
      "             ReLU-13           [-1, 64, 96, 96]               0\n",
      "     GroupNormAct-14           [-1, 64, 96, 96]             128\n",
      "    StdConv2dSame-15           [-1, 64, 96, 96]          36,864\n",
      "         Identity-16           [-1, 64, 96, 96]               0\n",
      "             ReLU-17           [-1, 64, 96, 96]               0\n",
      "     GroupNormAct-18           [-1, 64, 96, 96]             128\n",
      "    StdConv2dSame-19          [-1, 256, 96, 96]          16,384\n",
      "         Identity-20          [-1, 256, 96, 96]               0\n",
      "         Identity-21          [-1, 256, 96, 96]               0\n",
      "     GroupNormAct-22          [-1, 256, 96, 96]             512\n",
      "         Identity-23          [-1, 256, 96, 96]               0\n",
      "             ReLU-24          [-1, 256, 96, 96]               0\n",
      "       Bottleneck-25          [-1, 256, 96, 96]               0\n",
      "    StdConv2dSame-26           [-1, 64, 96, 96]          16,384\n",
      "         Identity-27           [-1, 64, 96, 96]               0\n",
      "             ReLU-28           [-1, 64, 96, 96]               0\n",
      "     GroupNormAct-29           [-1, 64, 96, 96]             128\n",
      "    StdConv2dSame-30           [-1, 64, 96, 96]          36,864\n",
      "         Identity-31           [-1, 64, 96, 96]               0\n",
      "             ReLU-32           [-1, 64, 96, 96]               0\n",
      "     GroupNormAct-33           [-1, 64, 96, 96]             128\n",
      "    StdConv2dSame-34          [-1, 256, 96, 96]          16,384\n",
      "         Identity-35          [-1, 256, 96, 96]               0\n",
      "         Identity-36          [-1, 256, 96, 96]               0\n",
      "     GroupNormAct-37          [-1, 256, 96, 96]             512\n",
      "         Identity-38          [-1, 256, 96, 96]               0\n",
      "             ReLU-39          [-1, 256, 96, 96]               0\n",
      "       Bottleneck-40          [-1, 256, 96, 96]               0\n",
      "      ResNetStage-41          [-1, 256, 96, 96]               0\n",
      "    StdConv2dSame-42          [-1, 512, 48, 48]         131,072\n",
      "         Identity-43          [-1, 512, 48, 48]               0\n",
      "         Identity-44          [-1, 512, 48, 48]               0\n",
      "     GroupNormAct-45          [-1, 512, 48, 48]           1,024\n",
      "   DownsampleConv-46          [-1, 512, 48, 48]               0\n",
      "    StdConv2dSame-47          [-1, 128, 96, 96]          32,768\n",
      "         Identity-48          [-1, 128, 96, 96]               0\n",
      "             ReLU-49          [-1, 128, 96, 96]               0\n",
      "     GroupNormAct-50          [-1, 128, 96, 96]             256\n",
      "    StdConv2dSame-51          [-1, 128, 48, 48]         147,456\n",
      "         Identity-52          [-1, 128, 48, 48]               0\n",
      "             ReLU-53          [-1, 128, 48, 48]               0\n",
      "     GroupNormAct-54          [-1, 128, 48, 48]             256\n",
      "    StdConv2dSame-55          [-1, 512, 48, 48]          65,536\n",
      "         Identity-56          [-1, 512, 48, 48]               0\n",
      "         Identity-57          [-1, 512, 48, 48]               0\n",
      "     GroupNormAct-58          [-1, 512, 48, 48]           1,024\n",
      "         Identity-59          [-1, 512, 48, 48]               0\n",
      "             ReLU-60          [-1, 512, 48, 48]               0\n",
      "       Bottleneck-61          [-1, 512, 48, 48]               0\n",
      "    StdConv2dSame-62          [-1, 128, 48, 48]          65,536\n",
      "         Identity-63          [-1, 128, 48, 48]               0\n",
      "             ReLU-64          [-1, 128, 48, 48]               0\n",
      "     GroupNormAct-65          [-1, 128, 48, 48]             256\n",
      "    StdConv2dSame-66          [-1, 128, 48, 48]         147,456\n",
      "         Identity-67          [-1, 128, 48, 48]               0\n",
      "             ReLU-68          [-1, 128, 48, 48]               0\n",
      "     GroupNormAct-69          [-1, 128, 48, 48]             256\n",
      "    StdConv2dSame-70          [-1, 512, 48, 48]          65,536\n",
      "         Identity-71          [-1, 512, 48, 48]               0\n",
      "         Identity-72          [-1, 512, 48, 48]               0\n",
      "     GroupNormAct-73          [-1, 512, 48, 48]           1,024\n",
      "         Identity-74          [-1, 512, 48, 48]               0\n",
      "             ReLU-75          [-1, 512, 48, 48]               0\n",
      "       Bottleneck-76          [-1, 512, 48, 48]               0\n",
      "      ResNetStage-77          [-1, 512, 48, 48]               0\n",
      "    StdConv2dSame-78         [-1, 1024, 24, 24]         524,288\n",
      "         Identity-79         [-1, 1024, 24, 24]               0\n",
      "         Identity-80         [-1, 1024, 24, 24]               0\n",
      "     GroupNormAct-81         [-1, 1024, 24, 24]           2,048\n",
      "   DownsampleConv-82         [-1, 1024, 24, 24]               0\n",
      "    StdConv2dSame-83          [-1, 256, 48, 48]         131,072\n",
      "         Identity-84          [-1, 256, 48, 48]               0\n",
      "             ReLU-85          [-1, 256, 48, 48]               0\n",
      "     GroupNormAct-86          [-1, 256, 48, 48]             512\n",
      "    StdConv2dSame-87          [-1, 256, 24, 24]         589,824\n",
      "         Identity-88          [-1, 256, 24, 24]               0\n",
      "             ReLU-89          [-1, 256, 24, 24]               0\n",
      "     GroupNormAct-90          [-1, 256, 24, 24]             512\n",
      "    StdConv2dSame-91         [-1, 1024, 24, 24]         262,144\n",
      "         Identity-92         [-1, 1024, 24, 24]               0\n",
      "         Identity-93         [-1, 1024, 24, 24]               0\n",
      "     GroupNormAct-94         [-1, 1024, 24, 24]           2,048\n",
      "         Identity-95         [-1, 1024, 24, 24]               0\n",
      "             ReLU-96         [-1, 1024, 24, 24]               0\n",
      "       Bottleneck-97         [-1, 1024, 24, 24]               0\n",
      "    StdConv2dSame-98          [-1, 256, 24, 24]         262,144\n",
      "         Identity-99          [-1, 256, 24, 24]               0\n",
      "            ReLU-100          [-1, 256, 24, 24]               0\n",
      "    GroupNormAct-101          [-1, 256, 24, 24]             512\n",
      "   StdConv2dSame-102          [-1, 256, 24, 24]         589,824\n",
      "        Identity-103          [-1, 256, 24, 24]               0\n",
      "            ReLU-104          [-1, 256, 24, 24]               0\n",
      "    GroupNormAct-105          [-1, 256, 24, 24]             512\n",
      "   StdConv2dSame-106         [-1, 1024, 24, 24]         262,144\n",
      "        Identity-107         [-1, 1024, 24, 24]               0\n",
      "        Identity-108         [-1, 1024, 24, 24]               0\n",
      "    GroupNormAct-109         [-1, 1024, 24, 24]           2,048\n",
      "        Identity-110         [-1, 1024, 24, 24]               0\n",
      "            ReLU-111         [-1, 1024, 24, 24]               0\n",
      "      Bottleneck-112         [-1, 1024, 24, 24]               0\n",
      "     ResNetStage-113         [-1, 1024, 24, 24]               0\n",
      "   StdConv2dSame-114         [-1, 2048, 12, 12]       2,097,152\n",
      "        Identity-115         [-1, 2048, 12, 12]               0\n",
      "        Identity-116         [-1, 2048, 12, 12]               0\n",
      "    GroupNormAct-117         [-1, 2048, 12, 12]           4,096\n",
      "  DownsampleConv-118         [-1, 2048, 12, 12]               0\n",
      "   StdConv2dSame-119          [-1, 512, 24, 24]         524,288\n",
      "        Identity-120          [-1, 512, 24, 24]               0\n",
      "            ReLU-121          [-1, 512, 24, 24]               0\n",
      "    GroupNormAct-122          [-1, 512, 24, 24]           1,024\n",
      "   StdConv2dSame-123          [-1, 512, 12, 12]       2,359,296\n",
      "        Identity-124          [-1, 512, 12, 12]               0\n",
      "            ReLU-125          [-1, 512, 12, 12]               0\n",
      "    GroupNormAct-126          [-1, 512, 12, 12]           1,024\n",
      "   StdConv2dSame-127         [-1, 2048, 12, 12]       1,048,576\n",
      "        Identity-128         [-1, 2048, 12, 12]               0\n",
      "        Identity-129         [-1, 2048, 12, 12]               0\n",
      "    GroupNormAct-130         [-1, 2048, 12, 12]           4,096\n",
      "        Identity-131         [-1, 2048, 12, 12]               0\n",
      "            ReLU-132         [-1, 2048, 12, 12]               0\n",
      "      Bottleneck-133         [-1, 2048, 12, 12]               0\n",
      "   StdConv2dSame-134          [-1, 512, 12, 12]       1,048,576\n",
      "        Identity-135          [-1, 512, 12, 12]               0\n",
      "            ReLU-136          [-1, 512, 12, 12]               0\n",
      "    GroupNormAct-137          [-1, 512, 12, 12]           1,024\n",
      "   StdConv2dSame-138          [-1, 512, 12, 12]       2,359,296\n",
      "        Identity-139          [-1, 512, 12, 12]               0\n",
      "            ReLU-140          [-1, 512, 12, 12]               0\n",
      "    GroupNormAct-141          [-1, 512, 12, 12]           1,024\n",
      "   StdConv2dSame-142         [-1, 2048, 12, 12]       1,048,576\n",
      "        Identity-143         [-1, 2048, 12, 12]               0\n",
      "        Identity-144         [-1, 2048, 12, 12]               0\n",
      "    GroupNormAct-145         [-1, 2048, 12, 12]           4,096\n",
      "        Identity-146         [-1, 2048, 12, 12]               0\n",
      "            ReLU-147         [-1, 2048, 12, 12]               0\n",
      "      Bottleneck-148         [-1, 2048, 12, 12]               0\n",
      "     ResNetStage-149         [-1, 2048, 12, 12]               0\n",
      "        Identity-150         [-1, 2048, 12, 12]               0\n",
      "        Identity-151         [-1, 2048, 12, 12]               0\n",
      "        Identity-152         [-1, 2048, 12, 12]               0\n",
      "SelectAdaptivePool2d-153         [-1, 2048, 12, 12]               0\n",
      "         Dropout-154         [-1, 2048, 12, 12]               0\n",
      "        Identity-155         [-1, 2048, 12, 12]               0\n",
      "        Identity-156         [-1, 2048, 12, 12]               0\n",
      "  ClassifierHead-157         [-1, 2048, 12, 12]               0\n",
      "        ResNetV2-158         [-1, 2048, 12, 12]               0\n",
      "          Conv2d-159          [-1, 384, 12, 12]         786,816\n",
      "     HybridEmbed-160             [-1, 144, 384]               0\n",
      "         Dropout-161             [-1, 145, 384]               0\n",
      "        Identity-162             [-1, 145, 384]               0\n",
      "        Identity-163             [-1, 145, 384]               0\n",
      "       LayerNorm-164             [-1, 145, 384]             768\n",
      "          Linear-165            [-1, 145, 1152]         443,520\n",
      "        Identity-166           [-1, 6, 145, 64]               0\n",
      "        Identity-167           [-1, 6, 145, 64]               0\n",
      "          Linear-168             [-1, 145, 384]         147,840\n",
      "         Dropout-169             [-1, 145, 384]               0\n",
      "       Attention-170             [-1, 145, 384]               0\n",
      "        Identity-171             [-1, 145, 384]               0\n",
      "        Identity-172             [-1, 145, 384]               0\n",
      "       LayerNorm-173             [-1, 145, 384]             768\n",
      "          Linear-174            [-1, 145, 1536]         591,360\n",
      "            GELU-175            [-1, 145, 1536]               0\n",
      "         Dropout-176            [-1, 145, 1536]               0\n",
      "          Linear-177             [-1, 145, 384]         590,208\n",
      "         Dropout-178             [-1, 145, 384]               0\n",
      "             Mlp-179             [-1, 145, 384]               0\n",
      "        Identity-180             [-1, 145, 384]               0\n",
      "        Identity-181             [-1, 145, 384]               0\n",
      "           Block-182             [-1, 145, 384]               0\n",
      "       LayerNorm-183             [-1, 145, 384]             768\n",
      "          Linear-184            [-1, 145, 1152]         443,520\n",
      "        Identity-185           [-1, 6, 145, 64]               0\n",
      "        Identity-186           [-1, 6, 145, 64]               0\n",
      "          Linear-187             [-1, 145, 384]         147,840\n",
      "         Dropout-188             [-1, 145, 384]               0\n",
      "       Attention-189             [-1, 145, 384]               0\n",
      "        Identity-190             [-1, 145, 384]               0\n",
      "        Identity-191             [-1, 145, 384]               0\n",
      "       LayerNorm-192             [-1, 145, 384]             768\n",
      "          Linear-193            [-1, 145, 1536]         591,360\n",
      "            GELU-194            [-1, 145, 1536]               0\n",
      "         Dropout-195            [-1, 145, 1536]               0\n",
      "          Linear-196             [-1, 145, 384]         590,208\n",
      "         Dropout-197             [-1, 145, 384]               0\n",
      "             Mlp-198             [-1, 145, 384]               0\n",
      "        Identity-199             [-1, 145, 384]               0\n",
      "        Identity-200             [-1, 145, 384]               0\n",
      "           Block-201             [-1, 145, 384]               0\n",
      "       LayerNorm-202             [-1, 145, 384]             768\n",
      "          Linear-203            [-1, 145, 1152]         443,520\n",
      "        Identity-204           [-1, 6, 145, 64]               0\n",
      "        Identity-205           [-1, 6, 145, 64]               0\n",
      "          Linear-206             [-1, 145, 384]         147,840\n",
      "         Dropout-207             [-1, 145, 384]               0\n",
      "       Attention-208             [-1, 145, 384]               0\n",
      "        Identity-209             [-1, 145, 384]               0\n",
      "        Identity-210             [-1, 145, 384]               0\n",
      "       LayerNorm-211             [-1, 145, 384]             768\n",
      "          Linear-212            [-1, 145, 1536]         591,360\n",
      "            GELU-213            [-1, 145, 1536]               0\n",
      "         Dropout-214            [-1, 145, 1536]               0\n",
      "          Linear-215             [-1, 145, 384]         590,208\n",
      "         Dropout-216             [-1, 145, 384]               0\n",
      "             Mlp-217             [-1, 145, 384]               0\n",
      "        Identity-218             [-1, 145, 384]               0\n",
      "        Identity-219             [-1, 145, 384]               0\n",
      "           Block-220             [-1, 145, 384]               0\n",
      "       LayerNorm-221             [-1, 145, 384]             768\n",
      "          Linear-222            [-1, 145, 1152]         443,520\n",
      "        Identity-223           [-1, 6, 145, 64]               0\n",
      "        Identity-224           [-1, 6, 145, 64]               0\n",
      "          Linear-225             [-1, 145, 384]         147,840\n",
      "         Dropout-226             [-1, 145, 384]               0\n",
      "       Attention-227             [-1, 145, 384]               0\n",
      "        Identity-228             [-1, 145, 384]               0\n",
      "        Identity-229             [-1, 145, 384]               0\n",
      "       LayerNorm-230             [-1, 145, 384]             768\n",
      "          Linear-231            [-1, 145, 1536]         591,360\n",
      "            GELU-232            [-1, 145, 1536]               0\n",
      "         Dropout-233            [-1, 145, 1536]               0\n",
      "          Linear-234             [-1, 145, 384]         590,208\n",
      "         Dropout-235             [-1, 145, 384]               0\n",
      "             Mlp-236             [-1, 145, 384]               0\n",
      "        Identity-237             [-1, 145, 384]               0\n",
      "        Identity-238             [-1, 145, 384]               0\n",
      "           Block-239             [-1, 145, 384]               0\n",
      "       LayerNorm-240             [-1, 145, 384]             768\n",
      "          Linear-241            [-1, 145, 1152]         443,520\n",
      "        Identity-242           [-1, 6, 145, 64]               0\n",
      "        Identity-243           [-1, 6, 145, 64]               0\n",
      "          Linear-244             [-1, 145, 384]         147,840\n",
      "         Dropout-245             [-1, 145, 384]               0\n",
      "       Attention-246             [-1, 145, 384]               0\n",
      "        Identity-247             [-1, 145, 384]               0\n",
      "        Identity-248             [-1, 145, 384]               0\n",
      "       LayerNorm-249             [-1, 145, 384]             768\n",
      "          Linear-250            [-1, 145, 1536]         591,360\n",
      "            GELU-251            [-1, 145, 1536]               0\n",
      "         Dropout-252            [-1, 145, 1536]               0\n",
      "          Linear-253             [-1, 145, 384]         590,208\n",
      "         Dropout-254             [-1, 145, 384]               0\n",
      "             Mlp-255             [-1, 145, 384]               0\n",
      "        Identity-256             [-1, 145, 384]               0\n",
      "        Identity-257             [-1, 145, 384]               0\n",
      "           Block-258             [-1, 145, 384]               0\n",
      "       LayerNorm-259             [-1, 145, 384]             768\n",
      "          Linear-260            [-1, 145, 1152]         443,520\n",
      "        Identity-261           [-1, 6, 145, 64]               0\n",
      "        Identity-262           [-1, 6, 145, 64]               0\n",
      "          Linear-263             [-1, 145, 384]         147,840\n",
      "         Dropout-264             [-1, 145, 384]               0\n",
      "       Attention-265             [-1, 145, 384]               0\n",
      "        Identity-266             [-1, 145, 384]               0\n",
      "        Identity-267             [-1, 145, 384]               0\n",
      "       LayerNorm-268             [-1, 145, 384]             768\n",
      "          Linear-269            [-1, 145, 1536]         591,360\n",
      "            GELU-270            [-1, 145, 1536]               0\n",
      "         Dropout-271            [-1, 145, 1536]               0\n",
      "          Linear-272             [-1, 145, 384]         590,208\n",
      "         Dropout-273             [-1, 145, 384]               0\n",
      "             Mlp-274             [-1, 145, 384]               0\n",
      "        Identity-275             [-1, 145, 384]               0\n",
      "        Identity-276             [-1, 145, 384]               0\n",
      "           Block-277             [-1, 145, 384]               0\n",
      "       LayerNorm-278             [-1, 145, 384]             768\n",
      "          Linear-279            [-1, 145, 1152]         443,520\n",
      "        Identity-280           [-1, 6, 145, 64]               0\n",
      "        Identity-281           [-1, 6, 145, 64]               0\n",
      "          Linear-282             [-1, 145, 384]         147,840\n",
      "         Dropout-283             [-1, 145, 384]               0\n",
      "       Attention-284             [-1, 145, 384]               0\n",
      "        Identity-285             [-1, 145, 384]               0\n",
      "        Identity-286             [-1, 145, 384]               0\n",
      "       LayerNorm-287             [-1, 145, 384]             768\n",
      "          Linear-288            [-1, 145, 1536]         591,360\n",
      "            GELU-289            [-1, 145, 1536]               0\n",
      "         Dropout-290            [-1, 145, 1536]               0\n",
      "          Linear-291             [-1, 145, 384]         590,208\n",
      "         Dropout-292             [-1, 145, 384]               0\n",
      "             Mlp-293             [-1, 145, 384]               0\n",
      "        Identity-294             [-1, 145, 384]               0\n",
      "        Identity-295             [-1, 145, 384]               0\n",
      "           Block-296             [-1, 145, 384]               0\n",
      "       LayerNorm-297             [-1, 145, 384]             768\n",
      "          Linear-298            [-1, 145, 1152]         443,520\n",
      "        Identity-299           [-1, 6, 145, 64]               0\n",
      "        Identity-300           [-1, 6, 145, 64]               0\n",
      "          Linear-301             [-1, 145, 384]         147,840\n",
      "         Dropout-302             [-1, 145, 384]               0\n",
      "       Attention-303             [-1, 145, 384]               0\n",
      "        Identity-304             [-1, 145, 384]               0\n",
      "        Identity-305             [-1, 145, 384]               0\n",
      "       LayerNorm-306             [-1, 145, 384]             768\n",
      "          Linear-307            [-1, 145, 1536]         591,360\n",
      "            GELU-308            [-1, 145, 1536]               0\n",
      "         Dropout-309            [-1, 145, 1536]               0\n",
      "          Linear-310             [-1, 145, 384]         590,208\n",
      "         Dropout-311             [-1, 145, 384]               0\n",
      "             Mlp-312             [-1, 145, 384]               0\n",
      "        Identity-313             [-1, 145, 384]               0\n",
      "        Identity-314             [-1, 145, 384]               0\n",
      "           Block-315             [-1, 145, 384]               0\n",
      "       LayerNorm-316             [-1, 145, 384]             768\n",
      "          Linear-317            [-1, 145, 1152]         443,520\n",
      "        Identity-318           [-1, 6, 145, 64]               0\n",
      "        Identity-319           [-1, 6, 145, 64]               0\n",
      "          Linear-320             [-1, 145, 384]         147,840\n",
      "         Dropout-321             [-1, 145, 384]               0\n",
      "       Attention-322             [-1, 145, 384]               0\n",
      "        Identity-323             [-1, 145, 384]               0\n",
      "        Identity-324             [-1, 145, 384]               0\n",
      "       LayerNorm-325             [-1, 145, 384]             768\n",
      "          Linear-326            [-1, 145, 1536]         591,360\n",
      "            GELU-327            [-1, 145, 1536]               0\n",
      "         Dropout-328            [-1, 145, 1536]               0\n",
      "          Linear-329             [-1, 145, 384]         590,208\n",
      "         Dropout-330             [-1, 145, 384]               0\n",
      "             Mlp-331             [-1, 145, 384]               0\n",
      "        Identity-332             [-1, 145, 384]               0\n",
      "        Identity-333             [-1, 145, 384]               0\n",
      "           Block-334             [-1, 145, 384]               0\n",
      "       LayerNorm-335             [-1, 145, 384]             768\n",
      "          Linear-336            [-1, 145, 1152]         443,520\n",
      "        Identity-337           [-1, 6, 145, 64]               0\n",
      "        Identity-338           [-1, 6, 145, 64]               0\n",
      "          Linear-339             [-1, 145, 384]         147,840\n",
      "         Dropout-340             [-1, 145, 384]               0\n",
      "       Attention-341             [-1, 145, 384]               0\n",
      "        Identity-342             [-1, 145, 384]               0\n",
      "        Identity-343             [-1, 145, 384]               0\n",
      "       LayerNorm-344             [-1, 145, 384]             768\n",
      "          Linear-345            [-1, 145, 1536]         591,360\n",
      "            GELU-346            [-1, 145, 1536]               0\n",
      "         Dropout-347            [-1, 145, 1536]               0\n",
      "          Linear-348             [-1, 145, 384]         590,208\n",
      "         Dropout-349             [-1, 145, 384]               0\n",
      "             Mlp-350             [-1, 145, 384]               0\n",
      "        Identity-351             [-1, 145, 384]               0\n",
      "        Identity-352             [-1, 145, 384]               0\n",
      "           Block-353             [-1, 145, 384]               0\n",
      "       LayerNorm-354             [-1, 145, 384]             768\n",
      "          Linear-355            [-1, 145, 1152]         443,520\n",
      "        Identity-356           [-1, 6, 145, 64]               0\n",
      "        Identity-357           [-1, 6, 145, 64]               0\n",
      "          Linear-358             [-1, 145, 384]         147,840\n",
      "         Dropout-359             [-1, 145, 384]               0\n",
      "       Attention-360             [-1, 145, 384]               0\n",
      "        Identity-361             [-1, 145, 384]               0\n",
      "        Identity-362             [-1, 145, 384]               0\n",
      "       LayerNorm-363             [-1, 145, 384]             768\n",
      "          Linear-364            [-1, 145, 1536]         591,360\n",
      "            GELU-365            [-1, 145, 1536]               0\n",
      "         Dropout-366            [-1, 145, 1536]               0\n",
      "          Linear-367             [-1, 145, 384]         590,208\n",
      "         Dropout-368             [-1, 145, 384]               0\n",
      "             Mlp-369             [-1, 145, 384]               0\n",
      "        Identity-370             [-1, 145, 384]               0\n",
      "        Identity-371             [-1, 145, 384]               0\n",
      "           Block-372             [-1, 145, 384]               0\n",
      "       LayerNorm-373             [-1, 145, 384]             768\n",
      "          Linear-374            [-1, 145, 1152]         443,520\n",
      "        Identity-375           [-1, 6, 145, 64]               0\n",
      "        Identity-376           [-1, 6, 145, 64]               0\n",
      "          Linear-377             [-1, 145, 384]         147,840\n",
      "         Dropout-378             [-1, 145, 384]               0\n",
      "       Attention-379             [-1, 145, 384]               0\n",
      "        Identity-380             [-1, 145, 384]               0\n",
      "        Identity-381             [-1, 145, 384]               0\n",
      "       LayerNorm-382             [-1, 145, 384]             768\n",
      "          Linear-383            [-1, 145, 1536]         591,360\n",
      "            GELU-384            [-1, 145, 1536]               0\n",
      "         Dropout-385            [-1, 145, 1536]               0\n",
      "          Linear-386             [-1, 145, 384]         590,208\n",
      "         Dropout-387             [-1, 145, 384]               0\n",
      "             Mlp-388             [-1, 145, 384]               0\n",
      "        Identity-389             [-1, 145, 384]               0\n",
      "        Identity-390             [-1, 145, 384]               0\n",
      "           Block-391             [-1, 145, 384]               0\n",
      "       LayerNorm-392             [-1, 145, 384]             768\n",
      "        Identity-393                  [-1, 384]               0\n",
      "         Dropout-394                  [-1, 384]               0\n",
      "          Linear-395                 [-1, 1000]         385,000\n",
      "================================================================\n",
      "Total params: 36,412,328\n",
      "Trainable params: 36,412,328\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.69\n",
      "Forward/backward pass size (MB): 1109.49\n",
      "Params size (MB): 138.90\n",
      "Estimated Total Size (MB): 1250.08\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = vit_small_r26_s32_384(pretrained=False).cuda()\n",
    "summary(model, (3, 384, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): HybridEmbed(\n",
       "    (backbone): ResNetV2(\n",
       "      (stem): Sequential(\n",
       "        (conv): StdConv2dSame(3, 64, kernel_size=(7, 7), stride=(2, 2), bias=False)\n",
       "        (norm): GroupNormAct(\n",
       "          32, 64, eps=1e-05, affine=True\n",
       "          (drop): Identity()\n",
       "          (act): ReLU(inplace=True)\n",
       "        )\n",
       "        (pool): MaxPool2dSame(kernel_size=(3, 3), stride=(2, 2), padding=(0, 0), dilation=(1, 1), ceil_mode=False)\n",
       "      )\n",
       "      (stages): Sequential(\n",
       "        (0): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 256, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 64, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 512, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 128, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 1024, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 256, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 1024, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (blocks): Sequential(\n",
       "            (0): Bottleneck(\n",
       "              (downsample): DownsampleConv(\n",
       "                (conv): StdConv2dSame(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (norm): GroupNormAct(\n",
       "                  32, 2048, eps=1e-05, affine=True\n",
       "                  (drop): Identity()\n",
       "                  (act): Identity()\n",
       "                )\n",
       "              )\n",
       "              (conv1): StdConv2dSame(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 2048, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "            (1): Bottleneck(\n",
       "              (conv1): StdConv2dSame(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm1): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv2): StdConv2dSame(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (norm2): GroupNormAct(\n",
       "                32, 512, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): ReLU(inplace=True)\n",
       "              )\n",
       "              (conv3): StdConv2dSame(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (norm3): GroupNormAct(\n",
       "                32, 2048, eps=1e-05, affine=True\n",
       "                (drop): Identity()\n",
       "                (act): Identity()\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (act3): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "      (head): ClassifierHead(\n",
       "        (global_pool): SelectAdaptivePool2d (pool_type=, flatten=Identity())\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (fc): Identity()\n",
       "        (flatten): Identity()\n",
       "      )\n",
       "    )\n",
       "    (proj): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=384, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet101c().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnetv2_s().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnetv2_m().cuda()\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "from dataset import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = A.Compose([\n",
    "    A.Resize(224, 224),\n",
    "    A.HorizontalFlip(),\n",
    "    A.Normalize(),\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(\n",
    "            \"/opt/ml/level3_cv_finalproject-cv-01/model\", is_train=True, tf=tf\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "            dataset=train_dataset,\n",
    "            batch_size=4,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            drop_last=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "\n",
    "cls_criterion = nn.CrossEntropyLoss()\n",
    "rcp_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels, ingredients in train_loader:\n",
    "        images, labels, ingredients = (\n",
    "                images.clone().detach().cuda(),\n",
    "                labels.clone().detach().cuda(),\n",
    "                ingredients.clone().detach().cuda(),\n",
    "            )\n",
    "        print(f\"image : {images.shape}\")\n",
    "        print(f\"label : {labels.shape}\")\n",
    "        print(f\"ingredient : {ingredients.shape}\")\n",
    "        print(f\"max : {torch.max(images)}, min : {torch.min(images)}\")\n",
    "\n",
    "        cls_output, rcp_output = model(images)\n",
    "        print(f\"cls_output : {cls_output.shape}\")\n",
    "        print(f\"rcp_output : {rcp_output.shape}\")\n",
    "        print(f\"max : {torch.max(cls_output)}, min : {torch.min(cls_output)}\")\n",
    "        print(f\"max : {torch.max(rcp_output)}, min : {torch.min(rcp_output)}\")\n",
    "\n",
    "        cls_output = F.sigmoid(cls_output)\n",
    "        rcp_output = F.sigmoid(rcp_output)\n",
    "        print(f\"max : {torch.max(cls_output)}, min : {torch.min(cls_output)}\")\n",
    "        print(f\"max : {torch.max(rcp_output)}, min : {torch.min(rcp_output)}\")\n",
    "        print(f\"max : {torch.max(labels)}, min : {torch.min(labels)}\")\n",
    "        print(f\"max : {torch.max(ingredients)}, min : {torch.min(ingredients)}\")\n",
    "\n",
    "        cls_loss = cls_criterion(cls_output, labels)\n",
    "        rcp_loss = rcp_criterion(rcp_output, ingredients)\n",
    "        print(f\"Loss : {cls_loss}, {rcp_loss}\")\n",
    "\n",
    "        cls_thr = 0.5\n",
    "        rcp_thr = 0.5\n",
    "        cls_output = (cls_output >= cls_thr).float()\n",
    "        rcp_output = (rcp_output >= cls_thr).float()\n",
    "        cls_loss = cls_criterion(cls_output, labels)\n",
    "        rcp_loss = rcp_criterion(rcp_output, ingredients)\n",
    "        print(f\"Loss : {cls_loss}, {rcp_loss}\")\n",
    "\n",
    "        cls_f1 = f1_score(y_pred=cls_output.cpu(), y_true=labels.cpu(), average=\"macro\", zero_division=0)\n",
    "        rcp_f1 = f1_score(y_pred=rcp_output.cpu(), y_true=ingredients.cpu(), average=\"macro\", zero_division=0)\n",
    "        cls_precision = precision_score(y_pred=cls_output.cpu(), y_true=labels.cpu(), average=\"macro\", zero_division=0)\n",
    "        rcp_precision = precision_score(y_pred=rcp_output.cpu(), y_true=ingredients.cpu(), average=\"macro\", zero_division=0)\n",
    "        print(f\"F1-Score : {cls_f1}, {rcp_f1}\")\n",
    "        print(f\"Precision Score : {cls_precision}, {rcp_precision}\")\n",
    "        raise\n",
    "        for i in range(images.shape[0]):\n",
    "            image = images[i].cpu()\n",
    "            image = image.permute(1, 2, 0)\n",
    "            image_np = image.numpy()\n",
    "            print(train_dataset.classes[labels[i].item()])\n",
    "            plt.imshow(image_np)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import cuda\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cuda\" if cuda.is_available() else \"cpu\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "max_epoch = 50\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=max_epoch, eta_min=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = []\n",
    "for epoch in range(max_epoch):\n",
    "    optimizer.zero_grad()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    lrs.append(current_lr)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(lrs)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "path = \"/opt/ml/level3_cv_finalproject-cv-01/model/data/json/milfeuille\"\n",
    "file_names = os.listdir(path)\n",
    "for file_name in file_names:\n",
    "    # new_name = file_name.replace(\"onirigi\", \"onigiri\")\n",
    "    with open(os.path.join(path, file_name)) as file:\n",
    "        data = json.load(file)\n",
    "    data['food_class'] = \"milfeuille\"\n",
    "    with open(os.path.join(\"/opt/ml/level3_cv_finalproject-cv-01/model/milfeuille\", file_name), \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
